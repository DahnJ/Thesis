\chapter{Simulation}
The Gibbs point process allows us a great flexibility in specifying the energy function. One of the disadvantages is that both simulating the GPP and estimating its parameters is computationally demanding. This chapter outlines the approach taken. This (and the following chapter) is a direct extension of \cite{DereudreLavancier2010} to the Laguerre case in three dimensions.
The principal issue in simulating GPP is that we do not know the value of the partition function $Z^z_\Lambda$. To that end, we emply Monte Chain Markov Carlo (MCMC) techniques.

\section{Monte Chain Markov Carlo}
\todoo[inline]{If there's time left, revisit this chapter and properly understand the stuff}
Before formulating the algorithm used to simulate our models, we first present some basic theory of Markov chains and their use in Monte Carlo techniques. For an introduction to these techniques with respect to point processes with density, see chapter $7$ in \cite{MollerWaagepetersen2003}. For a more comprehensive text, we refer to \cite{RobertCasella2004} or \cite{MeynTweedie1993}.

\subsection{Basic notions}
We first define the basic terms to do with general state-space Markov chains.

\begin{definition}
A measurable mapping $P:\Omega\times \mathcal A \to [0,1]$ such that
\begin{enumerate}
\item for each $B \in \mathcal A$, $P(\cdot, B)$ is a non-negative measurable function on $\Omega$,
\item for each $x \in \Omega$, $P(x,\cdot)$ is a probability measure on $(\Omega,\mathcal A)$
\end{enumerate}
is called a \textit{probability kernel} on $(\Omega, \mathcal A)$.
\end{definition}

\begin{definition} A stochastic process $Y=\{Y_n,n \in \mathbb N_0\}$ defined on $(\Omega,\mathcal A)$ is called a \textit{time-homogenous Markov chain} with \textit{initial distribution} $\mu$ and \textit{transition probability kernel} $P(x,A),x\in\Omega,A \in \mathcal A$, if for any $n\in \mathbb N_0$ and any sets $A_0,\dots,A_n$ we have
$$P_\mu(Y_0\in A_0,\dots, Y_n \in A_n) = \int_{A_0} \cdots \int_{A_{n-1}} P(y_{n-1},A_n) P(y_{n-2},dy_{n-1}) \cdots P(y_0,dy_1)\mu(dy_0)$$
where $P_\mu(B), B \in \bigotimes^\infty_{n=0} \mathcal A$ is the probability of the event $[Y\in B]$.
\end{definition}

Such process exists by Theorem $3.4.1$ in \cite{MeynTweedie1993} if $\mathcal A$ is generated by a countable collection of sets. This is true for $\mathcal N_{lf}$, see proposition B$.1$ in \cite{MollerWaagepetersen2003}. \newline

The definition suggests that the probability kernel $P(x,A)$ can be interpreted as a the probability that $Y_{m+1} \in A$ given that $Y_{m}=x$. Note that the probability is independent of $m$, which motivates the name \textit{time-homogenous}.

Next we iteratively define the \textit{m-step transition probability}. Set $P^0(x,A) = \delta_x(A)$ and for $n\geq 1$ define
$$P^m(x,A) = \int_\Omega P(y,A) P^{m-1} (x,dy)$$


In the following, let $Y=\{Y_n, n\in \mathbb N_0\}$ always be a Markov chain and $\pi$ a probability distribution on $(\Omega,\mathcal A)$.

\begin{definition}
A Markov chain $\{Y_n, n \in \mathbb N_0\}$ 
\begin{enumerate}
\item has an \textit{invariant distribution} $\mathbf \pi$ if $Y_m \sim \mathbf\pi$ implies $Y_{m+1}\sim \mathbf\pi$. In the integral form
$$\int_\Omega P(x,A) \mathbf\pi(dx) = \int_A \mathbf\pi(dx).$$
\item is \textit{reversible} with respect to the distribution $\mathbf \pi$ if $Y_m \sim \mathbf\pi$ then $(Y_m,Y_{m+1})$ and $(Y_{m+1},Y_m)$ are identically distributed. In the integral form
$$\int_B P(x,A) \mathbf\pi(dx) = \int_A P(x,B) \mathbf\pi(dx).$$
\item is ($\psi$)-\textit{irreducible} if ther exists a nonzero measure $\psi$ such that for any $x\in \Omega, A\in\mathcal A$ with $\psi(A)>0$ we have $P^m(x,A)>0$ for some $m\in\mathbb N_0$.
\end{enumerate}
\end{definition}
From the definition it is immediately observable that if $Y$ is reversible with respect to $\pi$, then $\pi$ is also its invariant distribiution.


\begin{definition} Let $Y$ be $psi$-irreducible. Then we call $Y$ \textit{periodic} if there exists a partitioning $D_0,\dots,D_{d-1},A$ of $\Omega$ such that $\psi(A)=0$ and 
$$P(x,D_{j} = 1 \; x\in D_i\; j=(i+1)\mathrm{mod} d$$
with $d>1$. In the opposite case $Y$ is \textit{aperiodic}
\end{definition}
The partitioning always exists for an irreducible Markov chain $Y$ by Theorem $5.4.4$ in \cite{MeynTweedie1993}.
\todoo[inline]{There needs to be more about periodicity, 129 in MW}




To measure the distance between two probability distribution, we recall the definition of the total variation norm.
\begin{definition} 
Let $\mu,\nu$ be two probability distributions on $\Omega$. Then we define the \textit{total variation norm} by 
$$\| \mu -\nu\|_{TV} = \sup_{F \subset \Omega} |\mu(F) - \nu(G)|$$
\end{definition}
\todoo[inline]{Some notes, in particular implication of weak convergence}


\begin{definition} We say that $\pi$ is a \textit{limiting distribution} of $Y$ if there exists $A\in\mathcal A, \pi(A)=0$ such that
$$\lim_{m\to \infty} \|P^m(x,\cdot) - \pi\|_{TV} = 0$$
for all $x \in \Omega \setminus A$.
\end{definition}



\begin{proposition} Let $Y$ be irreducible and $\pi$ its invariant distribution. Then $\pi$ is the unique invariant distribution (up to null sets).
\end{proposition}
\begin{proof}
Proposition $7.2$ in \cite{MollerWaagepetersen2003}
\end{proof}

The key proposition
\begin{proposition} Let $Y$ be irreducible  and aperiodic and $\pi$ its invariant distribution. Then $\pi$ is also the limiting distribution of $Y$. 
\end{proposition}
\begin{proof}
Proposition $7.7$ in \cite{MollerWaagepetersen2003}.
\end{proof}



Consider a probability distribution $\mathbf \pi$ be a distribution on some measurable space $(\Omega,\mathcal A)$. We wish to construct a Markov Chain $Y$ on $\Omega$ with its stationary distribution equal to $\mathbf \pi$.




\subsection{Birth-Death-Move Metropolis-Hastings algorithm}
In the last section we outlined how Markov chains may be used to sample from a distribution. 

We first describe the algorithm in general, adapted from \cite{MollerWaagepetersen2003}.

Let $\Omega$ be the state space. A natural choice is $\Omega=N_f \cap N_\infty$. \newline

We first introduce the quantities from the algorithm. Let $\gamma = \{x_1,\dots, x_n\} \in \Omega$ be the current state, denote $\bar\gamma = (x_1,\dots, x_n)$.

\begin{tabular}{ll}
$p(\gamma)$ & Probability of birth proposal  \\
$q_i(\bar\gamma,\cdot)$ & Density for the location of the point replacing the point $x_i$. \\
$q_b(\gamma,\cdot)$ & Density for the location of the point at birth proposal. \\
$q_d(\gamma,\cdot)$ & Density for the selection of the point at death proposal. 
\end{tabular}

Finally we introduce the so called \textit{Hasting ratios}
\begin{align*}
r_i(\bar \gamma,y) &= \frac{f(\gamma \setminus \{x_i\} \cup \{y\}) q_i((x_1,\dots,x_{i-1},y,x_{i+1},\dots,x_n), x_i) }{f(\gamma)q_i(\bar\gamma,y)} \\
r_b(\gamma,x) &= \frac{f(\gamma \cup \{x\})(1-p(\gamma \cup \{x\})) q_d(\gamma \cup \{x\}, \{x\})}{f(\gamma)p(\gamma)q_b(\gamma,x)} \\
r_d(\gamma,x) &= \frac{f(\gamma \setminus \{x\})p(\gamma \setminus\{x\}) q_b(\gamma \setminus \{x\}, x)}{f(\gamma)(1-p(\gamma))q_d(\gamma,x)}
\end{align*}
with the convention $a/0=1$ for $a\geq 0$.

\textbf{Algorithm A}\newline
\noindent Let $\gamma_0 \in \Omega$ be an initial configuration. 
For $m=0,1,\dots$ given $\gamma_m \in N_f$, generate $\gamma_{m+1}$ as follows
\begin{enumerate} 
	\item Generate $b$ and $r_m$ independently and uniformly on $[0,1]$.
	\item If $r_m \leq q$, then set $\bar\gamma_m=(x_1,\dots, x_n)$, generate $i$ uniformly on $\{1,\dots, n\}$, generate $y \sim q_i(\bar \gamma_m,\cdot)$ and set 
		\begin{equation}\label{move}
		\gamma_{m+1} = 
		\left\{
		    \begin{array}{ll}
			\gamma_m \setminus\{x_i\}\cup \{y\} & \mbox{if }  b < r_i(\bar\gamma_m,y)\\ 
			\gamma_m & \mbox{otherwise. }
		    \end{array}
		\right. 
		\end{equation}
	\item If $r_m > q$, perform the birth/deah step. 
	\begin{enumerate}
		\item Generate $r_{b}$ uniformly on $[0,1]$.
		    \item If $r_{b}\leq p(\gamma_m)$, then generate $x\sim q_b(\gamma_m,\cdot)$ and set
			\begin{equation}\label{birth}
			\gamma_{m+1} = 
			\left\{
			    \begin{array}{ll}
				\gamma_m \cup \{x\} & \mbox{if }  b < r_b(\gamma_m, x) \\
				\gamma_m & \mbox{otherwise. }
			    \end{array}
			\right. 
			\end{equation}
		    \item If $r_{b}>p(\gamma_m)$, then generate $x\sim q_d(\gamma_m,\cdot)$ and set
			\begin{equation}\label{death}
			\gamma_{m+1} = 
			\left\{
			    \begin{array}{ll}
				\gamma_m \setminus \{x\} & \mbox{if }  b < r_d(\gamma_m, x) \\ 
				\gamma_m & \mbox{otherwise. }
			    \end{array}
			\right. 
			\end{equation}
	\end{enumerate}
\end{enumerate}





Natural choice is $\Omega = N_\infty$.


\begin{proposition}
The Markov chain generated by algorithm [ref] is
\begin{enumerate}
\item reversible with respect to $h$,
\item $\Psi$-irreducible and aperiodic if  the following conditions are satisfied
$$p(\emptyset)<1$$
For all $\gamma \in E, \gamma\neq \emptyset$ exists $x \in \gamma$ such that
$$(1-p(\gamma))q_d(\gamma,x)>0 \text{ and } h(\gamma\setminus \{x\})p(\gamma \setminus \{x\}) q_b(\gamma\setminus \{x\},x)$$ 
\end{enumerate}
\end{proposition}
\begin{proof}
Proposition $7.15$ in \cite{MollerWaagepetersen2003}.
\end{proof}








\section{Simulating Gibbs-Laguerre-Delaunay tessellations}
\subsection{Definition of the models}
Our goal is to simulate examples of both smooth-interaction and hardcore-interaction potentials defined in section \ref{sec:Existence} on $\mathcal D$ and $\mathcal {LD}$ models. To that end, we choose a particular form of the potentials.

\begin{equation}\label{modelHC}
\varphi^{\theta,\alpha}_{HC}(\eta,\x) = 
\left\{
    \begin{array}{ll}
        \infty & \mbox{if } \delta(\eta)> \alpha, \\
        \theta \mathrm{Sur}(\eta) & \mbox{otherwise, }
    \end{array}
\right. 
\end{equation}

\begin{equation}\label{modelS}
\varphi^{\theta}_{S}(\eta,\x) =  \theta \mathrm{Sur}(\eta) 
\end{equation}

where $\mathrm{Sur}(\eta)$ is the surface area of $\mathrm{conv}(\eta)$.





\subsection{Simulation algorithm}

\todoo[inline]{Need non-redundant states! Measurability?}
State space
$$E=N_\infty \cap N_{nr}$$


First, start from a permissible initial configuration $\gamma_0$.
\begin{enumerate}
    \item Let $n = N_\Lambda(\gamma)$.
    \item Draw independently $r$ and $b$ uniformly on $[0,1]$.
    \item If $r<1/3$, then generate $x$ uniformly on $A$ and set
        \begin{equation}\label{birth}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \cup \{x\} & \mbox{if }  b < \frac{z f(\gamma_0 \cup \{x\})}{(n+1)f(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item If $r>2/3$, then generate $x$ uniformly on $\gamma_0$ and set
        \begin{equation}\label{death}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \setminus \{x\} & \mbox{if }  b < \frac{n f(\gamma_0 \setminus \{x\})}{zf(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item If $1/3 < r < 2/3$, then generate $x$ uniformly on $\gamma_0$, generate $y\sim \mathcal N (x, \sigma^2 I)$ such that $y \in A$ and set
        \begin{equation}\label{move}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \setminus \{x\} \cup \{y\} & \mbox{if }  b < \frac{f(\gamma_0 \setminus \{x\} \cup \{y\})}{f(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item Set $\gamma_0 \leftarrow \gamma_1$ and go to 1.
\end{enumerate}
\problem[inline]{This is not true at the moment, currently the algorithm just skips points in conflict with other points.}
\unsure[inline]{Why does this work? Can we choose different proposal density to improve the convergence?}

If the moved point would fall outside $[0,1]^3$, it is \unsure{Is this a good approach? I makes the density of moved points next to boundary concentrate more in some places} \todoo{Describe this properly}'bounced back' from the boundary of $[0,1]^3$ as if the boundary of the unit box was a solid wall. 
This differs from~\cite{DereudreLavancier2010}, where the point would be replaced inside $[0,1]^2$ by the periodic property. 
The idea is that a small perturbation to the point's position should not result in a radically different position of the point. 

\begin{remark}[Algorithm A]

\end{remark}

\subsection{Simplified form of proposal densities}
The Hastings ratios require us to calculate a ratio of densities $f$ both containing the energy function. Such calculation would be lengthy and would render the whole approach infeasible. However, here again the locality of the tetrahedrization allows us to express the Hastings ratios with only those tetrahedra which are affected by the added, removed, or moved point.

The ratio of densities in birth step \ref{birth} then becomes:
\begin{align*}
\frac{f(\gamma_0 \cup\{x\})}{f(\gamma_0)} &= \exp\left({\sum_{\eta\in \mathcal E_\Lambda(\gamma_0 \cup\{x\})} \varphi(\eta,\gamma_0 \cup\{x\}) - \sum_{\eta\in \mathcal E_\Lambda(\gamma_0)}\varphi(\eta,\gamma_0)}\right) \\
&= \exp\left(  \sum_{T \in \mathcal {LD}^\otimes (x,\gamma_0)} \varphi(\eta,\gamma_0)  - \sum_{T\in \mathcal {LD}^\ell (x,\gamma_0 \cup\{x\})} \varphi(\eta,\gamma_0 \cup\{x\}) \right)  
\end{align*}

Ratio for death step \ref{death} becomes:
\begin{align*}
\frac{f(\gamma_0 \setminus\{x\})}{f(\gamma_0)}&= \exp\left({\sum_{\eta\in \mathcal E_\Lambda(\gamma_0 \setminus\{x\})} \varphi(\eta,\gamma_0 \setminus\{x\})- \sum_{\eta\in \mathcal E_\Lambda(\gamma_0)}\varphi(\eta,\gamma_0)}\right)\\
&= \exp\left( \sum_{T\in \mathcal {LD}^\ell (x,\gamma_0)} \varphi(\eta,\gamma_0) - \sum_{T \in \mathcal {LD}^\otimes (x,\gamma_0 \setminus\{x\} )} \varphi(\eta,\gamma_0 \setminus\{x\})   \right)
\end{align*}

Ratio for move step \ref{move} becomes:
\begin{align*}
& \frac{f(\gamma_0 \setminus\{x\} \cup\{y\})}{f(\gamma_0)}= 
\frac{f(\gamma_0 \setminus\{x\} \cup\{y\})}{f(\gamma_0 \setminus\{x\})} \frac{f(\gamma_0 \setminus\{x\})}{f(\gamma_0)} \\ 
&= \exp \Bigg(  \sum_{T \in \mathcal {LD}^\otimes (y,\gamma_0 \setminus\{x\})} \varphi(\eta,\gamma_0 \setminus\{x\})  - \sum_{T\in \mathcal {LD}^\ell (y,\gamma_0 \setminus\{x\} \cup\{y\})} \varphi(\eta,\gamma_0 \setminus\{x\} \cup\{y\})  \\
&+ \sum_{T\in \mathcal {LD}^\ell (x,\gamma_0)} \varphi(\eta,\gamma_0) - \sum_{T \in \mathcal {LD}^\otimes (x,\gamma_0 \setminus\{x\})} \varphi(\eta,\gamma_0 \setminus\{x\}) \Bigg) \\
\end{align*}



These expressions simplify the energy calculation immensely. Whereas calculating the energy for the whole tessellation requires all the tetrahedra, and thus depends on the complexity of the sets $\mathcal E_\Lambda$, the final expressions only contain the tetrahedra local to $x$ through the sets of type $\mathcal {LD}^\otimes$ and $\mathcal {LD}^\ell$, and thus the energy can be calculated in constant time.


\subsection{Practical implementation}
All simulations were done in C++ using CGAL \cite{cgal},\cite{cgal:3d-triang}\todoo{Definitely sell this more later}. More details can be found in appendix \ref{appendix:implementation}.
\subsubsection{Initial configuration}
In~\cite{DereudreLavancier2010}, three options for the initial configuration are suggested: the empty configuration, a specific fixed outside configuration, and periodic configuration.
We ruled out periodic configuration since the CGAL implementation of 3d periodic triangulations~\cite{cgal:3d-period} has a much longer running time than in the non-periodic case. 
\cite{DereudreLavancier2010} rejects the empty configuration on the basis that it "produces non bounded Delaunay-Voronoi cells". While this is true for a Voronoi diagram, it does not hold for the Delaunay or Laguerre case and so such configuration would in fact be possible in our case.
However, the method chosen was to fix a regular grid of points in and out of $\Lambda$ such that the resulting tessellation fulfills the hardcore conditions. This does mean that the initial configuration is dependent on the values of $\alpha$ and $\epsilon$. 



\subsection{Irreducibility}
\tbd
