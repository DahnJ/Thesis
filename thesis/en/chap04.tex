\chapter{Simulation}\label{ch:simulation}
The Gibbs point process allows us a great flexibility in specifying the energy function. One of the disadvantages is that both simulating the GPP and estimating its parameters is computationally demanding. This chapter outlines the approach taken in simulating the GPP. 

This (and the following) chapter is a direct extension of \cite{DereudreLavancier2011}. Dereudre and Lavancier investigate random Voronoi tessellations and Delaunay triangulations on the plane $\mathbb R^2$ based on the Gibbs point process. This text attempts to generalize their results for the Delaunay triangulations to Laguerre tetrahedrizations, that is from $\mathbb R^2$ to $\mathbb R^3$ and from Delaunay to the Laguerre-Delaunay case.

\noindent The principal issue in simulating GPP is that we do not know the value of the partition function $Z^z_\Lambda$. To that end, we employ Monte chain Markov Carlo (MCMC) techniques. 

\section{Monte Chain Markov Carlo}
\todoo[inline]{If there's time left, revisit this chapter and properly understand the stuff}
Before formulating the algorithm used to simulate our models, we first present some basic theory of Markov chains and their use in Monte Carlo techniques. For an introduction to these techniques with point processes with density in mind, see Chapter $7$ in \cite{MollerWaagepetersen2003}. For a more general and comprehensive text, we refer to \cite{RobertCasella2004} or \cite{MeynTweedie1993}.

\subsection{Basic notions}
We first define the basic terms to do with general state-space Markov chains.

\begin{definition}
A measurable mapping $P:\Omega\times \mathcal A \to [0,1]$ such that
\begin{enumerate}
\item for each $B \in \mathcal A$, $P(\cdot, B)$ is a non-negative measurable function on $\Omega$,
\item for each $x \in \Omega$, $P(x,\cdot)$ is a probability measure on $\Omega$
\end{enumerate}
is called a \textit{probability kernel} on $(\Omega, \mathcal A)$.
\end{definition}

\begin{definition} A stochastic process $Y=\{Y_n,n \in \mathbb N_0\}$ defined on $(\Omega,\mathcal A)$ is called a \textit{time-homogeneous Markov chain} with \textit{initial distribution} $\mu$ and \textit{transition probability kernel} $P(x,A),x\in\Omega,A \in \mathcal A$, if for any $n\in \mathbb N_0$ and any sets $A_0,\dots,A_n$ we have
$$P_\mu(Y_0\in A_0,\dots, Y_n \in A_n) = \int_{A_0} \cdots \int_{A_{n-1}} P(y_{n-1},A_n) P(y_{n-2},dy_{n-1}) \cdots P(y_0,dy_1)\mu(dy_0)$$
where $P_\mu(B), B \in \bigotimes^\infty_{n=0} \mathcal A$ is the probability of the event $[Y\in B]$.
\end{definition}

Such process exists by Theorem $3.4.1$ in \cite{MeynTweedie1993} if $\mathcal A$ is generated by a countable collection of sets. This is true for $\mathcal N_{lf}$, see Proposition B$.1$ in \cite{MollerWaagepetersen2003}. \newline

The definition suggests that the probability kernel $P(x,A)$ can be interpreted as the probability that $Y_{m+1} \in A$ given that $Y_{m}=x$. Note that the probability is independent of $m$, which motivates the name \textit{time-homogeneous}.

Next we iteratively define the \textit{m-step transition probability}. Set $P^0(x,A) = \delta_x(A)$ and for $n\geq 1$ define
$$P^m(x,A) = \int_\Omega P(y,A) P^{m-1} (x,dy)$$


In the following, let $Y=\{Y_n, n\in \mathbb N_0\}$ always be a Markov chain and $\pi$ a probability distribution on $(\Omega,\mathcal A)$.

\begin{definition}
A Markov chain $\{Y_n, n \in \mathbb N_0\}$ 
\begin{enumerate}
\item has an \textit{invariant distribution} $\mathbf \pi$ if $Y_m \sim \mathbf\pi$ implies $Y_{m+1}\sim \mathbf\pi$. In the integral form
$$\int_\Omega P(x,A) \mathbf\pi(dx) = \int_A \mathbf\pi(dx), \quad A \in \mathcal A.$$
\item is \textit{reversible} with respect to the distribution $\mathbf \pi$ if $Y_m \sim \mathbf\pi$ then $(Y_m,Y_{m+1})$ and $(Y_{m+1},Y_m)$ are identically distributed. In the integral form
$$\int_B P(x,A) \mathbf\pi(dx) = \int_A P(x,B) \mathbf\pi(dx), \quad A,B\in\mathcal A .$$
\item is \textit{irreducible}, or $\psi-$\textit{irreducible} if there exists a nonzero measure $\psi$ such that for any $x~\in \Omega, A\in\mathcal A$ with $\psi(A)>0$ we have $P^m(x,A)>0$ for some $m\in\mathbb N_0$.
\end{enumerate}
\end{definition}
From the definition it is immediately observable that if $Y$ is reversible with respect to $\pi$, then $\pi$ is also its invariant distribution.



\begin{definition} Let $Y$ be $\psi$-irreducible. Then we call $Y$ \textit{periodic} if there exists a partitioning $D_0,\dots,D_{d-1},A$ of $\Omega$ such that $\psi(A)=0$ and 
$$P(x,D_{j}) = 1, \; x\in D_i\; j=(i+1)\; \mathrm{mod} \;d$$
with $d>1$. In the opposite case $Y$ is \textit{aperiodic}.
\end{definition}
The partitioning always exists for an irreducible Markov chain $Y$ by Theorem $5.4.4$ in \cite{MeynTweedie1993}. For the Markov chain to be aperiodic it is sufficient to have $P(x,\{x\})>0$ for some $x \in \Omega$.



To measure the distance between two probability distributions, we recall the definition of the total variation norm.
\begin{definition} 
Let $\mu,\nu$ be two probability distributions on $\Omega$. Then we define the \textit{total variation norm} by 
$$\| \mu -\nu\|_{TV} = \sup_{F \subset \Omega} |\mu(F) - \nu(G)|.$$
\end{definition}
Note that convergence in the total variation norm is quite a strong property and in particular implies weak convergence.


\begin{definition} We say that $\pi$ is a \textit{limiting distribution} of $Y$ if there exists $A\in\mathcal A, \pi(A)=0$ such that
$$\lim_{m\to \infty} \|P^m(x,\cdot) - \pi\|_{TV} = 0$$
for all $x \in \Omega \setminus A$.
\end{definition}



\begin{proposition}\label{prop:MC1} Let $Y$ be irreducible and $\pi$ its invariant distribution. Then $\pi$ is the unique invariant distribution (up to null sets).
\end{proposition}
\begin{proof}
Proposition $7.2$ in \cite{MollerWaagepetersen2003}
\end{proof}

\begin{proposition}\label{prop:MC2} Let $Y$ be irreducible  and aperiodic and $\pi$ its invariant distribution. Then $\pi$ is also the limiting distribution of $Y$. 
\end{proposition}
\begin{proof}
Proposition $7.7$ in \cite{MollerWaagepetersen2003}.
\end{proof}



Consider a probability distribution $\mathbf \pi$ on some measurable space $(\Omega,\mathcal A)$. We wish to construct a Markov chain $Y$ on $\Omega$ with its stationary distribution equal to $\mathbf \pi$. By the theory introduced in this section, we must construct an irreducible and aperiodic Markov chain reversible with respect to $\pi$. Then $\pi$ is also the unique invariant distribution and thus 
\unsure{unique?}the limiting distribution. Since convergence in total variation implies weak convergence, our task is complete.

The question, however, is how to build such a Markov chain. The next section answers that question. 



\subsection{Birth-Death-Move Metropolis-Hastings algorithm}
In the last section we outlined how Markov chains may be used to sample from a distribution in general. Imagine now that we wish to sample from a finite point process with the unnormalized density $f$ with respect to $\Pi_\Lambda$. In this section we introduce a variation of the Metropolis-Hastings algorithm for points processes with a density, the algorithm used to construct a Markov chain with the required properties. 

We first describe the algorithm in general, adapted from \cite{MollerWaagepetersen2003}.  \newline
\noindent Let $\Omega\subset \mathbf N_f$ be the state space. A natural choice is 
$$\Omega=\{\gamma \in \mathbf N_f: f(\gamma)>0 \} = \mathbf N_f \cap \mathbf N_\infty.$$
We first introduce the quantities to be used in the algorithm. Let $\gamma = \{x_1,\dots, x_n\} \in ~\Omega$ be the current state and denote $\bar\gamma = (x_1,\dots, x_n)$ the current state represented as a vector. We further denote 

\begin{tabular}{ll}
$p(\gamma)$ & Probability of a birth proposal if the current state is $\gamma$.  \\
$q_i(\bar\gamma,\cdot)$ & Density for the location of the point replacing the point $x_i$. \\
$q_b(\gamma,\cdot)$ & Density for the location of the point at birth proposal. \\
$q_d(\gamma,\cdot)$ & Density for the selection of the point at death proposal. 
\end{tabular}

\noindent Finally we introduce the so called \textit{Hasting ratios}
\begin{align*}
r_i(\bar \gamma,y) &= \frac{f(\gamma \setminus \{x_i\} \cup \{y\}) q_i((x_1,\dots,x_{i-1},y,x_{i+1},\dots,x_n), x_i) }{f(\gamma)q_i(\bar\gamma,y)}, \\
r_b(\gamma,x) &= \frac{f(\gamma \cup \{x\})(1-p(\gamma \cup \{x\})) q_d(\gamma \cup \{x\}, \{x\})}{f(\gamma)p(\gamma)q_b(\gamma,x)}, \\
r_d(\gamma,x) &= \frac{f(\gamma \setminus \{x\})p(\gamma \setminus\{x\}) q_b(\gamma \setminus \{x\}, x)}{f(\gamma)(1-p(\gamma))q_d(\gamma,x)},
\end{align*}
with the convention $a/0=1$ for $a\geq 0$.\newline

\noindent \textbf{Algorithm A}\newline
\noindent Let $\gamma_0 \in \Omega$ be an initial configuration. 
For $m=0,1,\dots$, given $\gamma_m \in N_f$, generate $\gamma_{m+1}$ as follows
\begin{enumerate} 
	\item Generate $b$ and $r_m$ independently and uniformly on $[0,1]$.
	\item If $r_m \leq q$, then set $\bar\gamma_m=(x_1,\dots, x_n)$, generate $i$ uniformly on $\{1,\dots, n\}$, generate $y \sim q_i(\bar \gamma_m,\cdot)$ and set 
		\begin{equation}
		\gamma_{m+1} = 
		\left\{
		    \begin{array}{ll}
			\gamma_m \setminus\{x_i\}\cup \{y\} & \mbox{if }  b < r_i(\bar\gamma_m,y)\\ 
			\gamma_m & \mbox{otherwise. }
		    \end{array}
		\right. 
		\end{equation}
	\item If $r_m > q$, perform the birth/death step. 
	\begin{enumerate}
		\item Generate $r_{b}$ uniformly on $[0,1]$.
		    \item If $r_{b}\leq p(\gamma_m)$, then generate $x\sim q_b(\gamma_m,\cdot)$ and set
			\begin{equation}
			\gamma_{m+1} = 
			\left\{
			    \begin{array}{ll}
				\gamma_m \cup \{x\} & \mbox{if }  b < r_b(\gamma_m, x) \\
				\gamma_m & \mbox{otherwise. }
			    \end{array}
			\right. 
			\end{equation}
		    \item If $r_{b}>p(\gamma_m)$ and $\gamma_m=\emptyset$ then set $\gamma_{m+1}=\emptyset$. Else if $\gamma_m\neq\emptyset$ generate $x\sim q_d(\gamma_m,\cdot)$ and set
			\begin{equation}
			\gamma_{m+1} = 
			\left\{
			    \begin{array}{ll}
				\gamma_m \setminus \{x\} & \mbox{if }  b < r_d(\gamma_m, x) \\ 
				\gamma_m & \mbox{otherwise. }
			    \end{array}
			\right. 
			\end{equation}
	\end{enumerate}
\end{enumerate}

\noindent The correctness of this approach is guaranteed by the following proposition.

\begin{proposition}\label{prop:algorithmconverges}
The Markov chain generated by Algorithm A is
\begin{enumerate}
\item reversible with respect to $f$,
\item $\Psi$-irreducible and aperiodic if the following conditions are satisfied
\begin{enumerate}[(i)]
	\item $p(\emptyset)<1,$
	\item for all $\gamma \in E, \gamma\neq \emptyset$, there exists $x \in \gamma$ such that
		\begin{equation}\label{eq:irreducibility}(1-p(\gamma))q_d(\gamma,x)>0 \text{ and } f(\gamma\setminus \{x\})p(\gamma \setminus \{x\}) q_b(\gamma\setminus \{x\},x)>0.\end{equation}
\end{enumerate}
\end{enumerate}
\end{proposition}
\begin{proof}
Proposition $7.15$ in \cite{MollerWaagepetersen2003}.
\end{proof}

Condition (i) ensures that the chain can remain in $\emptyset$. Condition (ii) ensures that a point can always be deleted and thus the chain can move from $\gamma$ to $\emptyset$ in $N_\Lambda(\gamma)$ steps.



\section{Simulating Gibbs-Laguerre-Delaunay tetrahedrizations} 
\subsection{Definition of the models}
Our goal is to simulate examples of both smooth-interaction and hardcore-interaction potentials defined in Section \ref{sec:Existence} on $\mathcal D$ and $\mathcal {LD}$ models. To that end, we choose a particular parametric form of potentials depending on the parameters $\theta$ and $\alpha$.

\begin{align}\label{modelHC}
&\varphi^{\theta,\alpha}_{HC}(\eta,\x) = 
\left\{
    \begin{array}{ll}
        \infty & \mbox{if } \delta(\eta)> \alpha, \\
        \theta \mathrm{Sur}(\eta) & \mbox{otherwise, }
    \end{array}
\right. \\
&\varphi^{\theta}_{S}(\eta,\x) =  \theta \mathrm{Sur}(\eta) 
\end{align}
where $\mathrm{Sur}(\eta)$ is the surface area of $\mathrm{conv}(\eta)$, $\alpha>0$, $\theta \in \mathbb R$. We then consider the models $(\mathcal D_4,\varphi^{\theta,\alpha}_{HC})$, $(\mathcal D_4,\varphi^{\theta}_{S})$, $(\mathcal {LD}_4,\varphi^{\theta,\alpha}_{HC})$ and $(\mathcal {LD}_4,\varphi^{\theta}_{S})$.



\subsection{Simulation algorithm}
We now present the variations of Algorithm A for our setting. The simulation window will be taken to be $[0,1]^3$ and we let $\Lambda \in \mathcal B_0(\Rt)$ be such that $[0,1]^3 \subset \Lambda$. All simulation takes place on $[0,1]^3$ and $\Lambda \setminus [0,1]^3$ contains the outside configuration, the boundary condition. 

The first algorithm simulates the Delaunay tetrahedrization. In the description, all unmarked points formally represent marked points with marks set to $0$. The state space thus is
$$\Omega = \{\gamma \in \mathbf N_f: f(\gamma)>0, \gamma \subset \Lambda\times \{0\}\},$$
the set of permissible configurations with zero marks.
\noindent \textbf{Algorithm A-$\mathcal D$} \newline
\noindent First, start from a permissible initial configuration $\gamma_0$.
\begin{enumerate}
	\item Let $n = N_{[0,1]^3}(\gamma)$.
    \item Draw independently $r$ and $b$ uniformly on $[0,1]$.
    \item If $r<1/3$, then generate $x$ uniformly on $[0,1]^3$ and set
        \begin{equation}\label{Dbirth}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \cup \{x\} & \mbox{if }  b < \frac{z f(\gamma_0 \cup \{x\})}{(n+1)f(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item If $r>2/3$, then generate $x$ uniformly on $\gamma_0$ and set
        \begin{equation}\label{Ddeath}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \setminus \{x\} & \mbox{if }  b < \frac{n f(\gamma_0 \setminus \{x\})}{zf(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item If $1/3 < r < 2/3$, then generate $x$ uniformly on $\gamma_0$, generate $y\sim \mathcal N (x, \sigma^2 I)$ such that $y \in [0,1]^3$ and set
        \begin{equation}\label{Dmove}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \setminus \{x\} \cup \{y\} & \mbox{if }  b < \frac{f(\gamma_0 \setminus \{x\} \cup \{y\})}{f(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item Set $\gamma_0 \leftarrow \gamma_1$ and go to 1.
\end{enumerate}

The second algorithm is for the simulation of the Laguerre tetrahedrization. The algorithm functions in the same way with one significant difference. In Section \ref{sec:redundant} we introduced the notion of redundant points. It would be inconvenient to have a newly added point render other points redundant and vice versa. To prevent that, we define the set of point configurations producing Laguerre-Delaunay hypergraph structures with no redundant points,
$$\mathbf N_{nr} = \{ \gamma \in \mathbf N_{lf}: \mathcal {LD}(\gamma) \text{ does not contain redundant points }\}.$$ 
Note that $\mathbf N_{nr}$ is \unsure{check this} measurable in $(\mathbf N_{lf}, \mathcal N_{lf})$ due to measurability of the hypergraph structures.  Using this set, we set the state space of the chain to
$$\Omega = \mathbf N_f \cap \mathbf N_\infty \cap \mathbf N_{nr}.$$
We further denote the set 
$$A_\gamma = \{ x \in \Lambda \times S: \gamma\cup\{x\} \in N_{nr}\},$$
the set of points which we can propose if the chain is in the state $\gamma$. Generation of points in $A_\gamma$ is done through rejection sampling, that is repeatedly sampling a point $x\in \Lambda\times S$ until the point satisfies $x\in A_\gamma$.
\unsure[inline]{Is the treatment of $\Lambda$ okay here?}

\noindent \textbf{Algorithm A-$\mathcal{LD}$} \newline
\noindent First, start from a permissible initial configuration $\gamma_0$.
\begin{enumerate}
	\item Let $n = N_{[0,1]^3}(\gamma_0)$.
    \item Draw independently $r$ and $b$ uniformly on $[0,1]$.
    \item If $r<1/3$, then generate $x$ uniformly on $A_{\gamma_0}$ and set
        \begin{equation}\label{birth}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \cup \{x\} & \mbox{if }  b < \frac{z f(\gamma_0 \cup \{x\})}{(n+1)f(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item If $r>2/3$, then generate $x$ uniformly on $\gamma_0$ and set
        \begin{equation}\label{death}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \setminus \{x\} & \mbox{if }  b < \frac{n f(\gamma_0 \setminus \{x\})}{zf(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item If $1/3 < r < 2/3$, then generate $x$ uniformly on $\gamma_0$, generate $y'\sim \mathcal N (x, \sigma^2 I)$ and $y''$ uniformly on $S$ such that $y=(y',y'') \in A_{\gamma_0 \setminus \{x\}}$ and set
        \begin{equation}\label{move}
        \gamma_1 = 
        \left\{
            \begin{array}{ll}
                \gamma_0 \setminus \{x\} \cup \{y\} & \mbox{if }  b < \frac{f(\gamma_0 \setminus \{x\} \cup \{y\})}{f(\gamma_0)}, \\
                \gamma_0 & \mbox{otherwise. }
            \end{array}
        \right. 
        \end{equation}
    \item Set $\gamma_0 \leftarrow \gamma_1$ and go to 1.
\end{enumerate}

\problem[inline]{This is not true at the moment, currently the algorithm just skips points in conflict with other points.}
\unsure[inline]{Why does this work? Can we choose different proposal density to improve the convergence?}


During the move step, the moved point might fall outside of $[0,1]^3$. In \cite{DereudreLavancier2011}, the point would be replaced by the periodic property. We do not use the periodic configuration (Section \ref{sec:practical}) and this approach would not be in line with the idea that a small perturbation to the point's position should not result in a radically different position. 
In our case the point is reflected back inside $[0,1]^3$, as if it was 'bounced back' from the boundary of $[0,1]^3$ \unsure{Is this a good approach? I makes the density of moved points next to boundary concentrate more in some places}.

\begin{remark}
Recall that the mark space $S=[0,W]$ limits the maximum weight. It is worth noting to what extent this is a limitation. From a practical perspective, the maximum weight $W$ limits the resulting tetrahedrization in the sense that the difference of weights can never be greater than $W$. Marks greater than $W$ are not necessarily a problem, as we can always find an identical tetrahedrization with marks bounded by $W$, as long as there are no two points $p,q$ with $|p''-q''|>W$ (see Remark \ref{rem:invariance}).
\end{remark}


\subsection{Simplified form of proposal densities}
The Hastings ratios require us to calculate a ratio of densities $f$ both containing the energy function. Such calculation would be lengthy and would render the whole approach unfeasible. However, here again the locality of the tetrahedrization allows us to express the Hastings ratios with only those tetrahedra which are affected by the added, removed, or moved point. If we recall Remark \ref{r:addremove}, we obtain the following simplifications for the ratios in Algorithm A-$\mathcal {LD}$.

The ratio of densities in birth step \eqref{birth} then becomes:
\begin{align*}
\frac{f(\gamma_0 \cup\{x\})}{f(\gamma_0)} &= \exp\left({\sum_{\eta\in \mathcal E_\Lambda(\gamma_0 \cup\{x\})} \varphi(\eta,\gamma_0 \cup\{x\}) - \sum_{\eta\in \mathcal E_\Lambda(\gamma_0)}\varphi(\eta,\gamma_0)}\right) \\
&= \exp\left(  \sum_{T \in \mathcal {LD}^\otimes (x,\gamma_0)} \varphi(\eta,\gamma_0)  - \sum_{T\in \mathcal {LD}^\ell (x,\gamma_0 \cup\{x\})} \varphi(\eta,\gamma_0 \cup\{x\}) \right)  
\end{align*}

Ratio for death step \eqref{death} becomes:
\begin{align*}
\frac{f(\gamma_0 \setminus\{x\})}{f(\gamma_0)}&= \exp\left({\sum_{\eta\in \mathcal E_\Lambda(\gamma_0 \setminus\{x\})} \varphi(\eta,\gamma_0 \setminus\{x\})- \sum_{\eta\in \mathcal E_\Lambda(\gamma_0)}\varphi(\eta,\gamma_0)}\right)\\
&= \exp\left( \sum_{T\in \mathcal {LD}^\ell (x,\gamma_0)} \varphi(\eta,\gamma_0) - \sum_{T \in \mathcal {LD}^\otimes (x,\gamma_0 \setminus\{x\} )} \varphi(\eta,\gamma_0 \setminus\{x\})   \right)
\end{align*}

Ratio for move step \eqref{move} becomes:
\begin{align*}
& \frac{f(\gamma_0 \setminus\{x\} \cup\{y\})}{f(\gamma_0)}= 
\frac{f(\gamma_0 \setminus\{x\} \cup\{y\})}{f(\gamma_0 \setminus\{x\})} \frac{f(\gamma_0 \setminus\{x\})}{f(\gamma_0)} \\ 
&= \exp \Bigg(  \sum_{T \in \mathcal {LD}^\otimes (y,\gamma_0 \setminus\{x\})} \varphi(\eta,\gamma_0 \setminus\{x\})  - \sum_{T\in \mathcal {LD}^\ell (y,\gamma_0 \setminus\{x\} \cup\{y\})} \varphi(\eta,\gamma_0 \setminus\{x\} \cup\{y\})  \\
&+ \sum_{T\in \mathcal {LD}^\ell (x,\gamma_0)} \varphi(\eta,\gamma_0) - \sum_{T \in \mathcal {LD}^\otimes (x,\gamma_0 \setminus\{x\})} \varphi(\eta,\gamma_0 \setminus\{x\}) \Bigg) \\
\end{align*}


These expressions simplify the energy calculation immensely. Whereas calculating the energy for the whole tetrahedrization requires all the tetrahedra, and thus depends on the complexity of the sets $\mathcal E_\Lambda$, the final expressions only contain the tetrahedra local to $x$ through the sets of type $\mathcal {LD}^\otimes$ and $\mathcal {LD}^\ell$, and thus the energy can be calculated in constant time.

We have presented the simplified ratios for Algorithm A-$\mathcal {LD}$. The case for Algorithm A-$\mathcal D$ is analogous, see Remark \ref{r:addremove}.

\subsection{Practical implementation}\label{sec:practical}
All simulations were done in C++ using CGAL \cite{cgal},\cite{cgal:3d-triang}\todoo{Definitely sell this more later}. More details can be found in Appendix \ref{appendix:implementation}.
\subsubsection{Initial configuration}
In~\cite{DereudreLavancier2011}, three options for the initial configuration are suggested: the empty configuration, a specific fixed outside configuration, and periodic configuration.
We decided against using the periodic configuration since the CGAL implementation of 3D periodic triangulations~\cite{cgal:3d-period} has a much longer running time than in the non-periodic case. 
\cite{DereudreLavancier2011} rejects the empty configuration on the basis that it "produces non bounded Delaunay-Voronoi cells". While this is true for a Voronoi diagram, it does not hold for the Delaunay or Laguerre case and so such configuration would in fact be possible in our case.
However, the method chosen was to fix a regular grid of points in and out of $\Lambda$ such that the resulting tessellation fulfills the hardcore conditions. This does mean that the initial configuration is dependent on the values of the hardcore parameter $\alpha$. 



\subsection{Convergence of the algorithm}\label{sec:convergence}
By Propositions \ref{prop:MC1} and \ref{prop:MC2}, the convergence of Algorithms \textbf{A-}$\mathcal D$ and \textbf{A-}$\mathcal {LD}$ requires the generated Markov chain to be irreducible, aperiodic, and reversible with respect to $f$\todoo{Reversible implies Invariant should be clearer here}. Reversibility is given by point $1$ in Proposition \ref{prop:algorithmconverges}. Aperiodicity is satisfied since the chain can stay in any state with non-zero probability. It only remains to verify the irreducibility.

For irreducibility, all that remains is to prove \eqref{eq:irreducibility}. This assumption is trivially satisfied in the models $(\mathcal D_4,\varphi_S)$ and $(\mathcal {LD}_4,\varphi_S)$. However, for the hard-core interaction models, this assumption is in general not satisfied, as the empty configuration does not even have to be in the state space, as it might not be permissible. In order to prove the irreducibility, \cite{DereudreLavancier2011} utilized an algorithmic procedure which connected any two configurations $\gamma,\bar\gamma \in \mathbf N_\infty$ by a finite series of specific steps of point addition and removal, all of which produce permissible configurations.

\todoo[inline]{Possibly mention second hard-core parameter, which might be necessary (which wouldn't be a problem for us)}
Although it seems very plausible that a similar approach could be taken in our case, we have not succeeded in extending this result to our case. In $\mathbb R^2$, the geometric arguments rely on simple relationships between the lengths of sides of the triangles and their circumradii which no longer work in $\Rt$. Furthermore, the extension to the Laguerre-Delaunay case adds an additional constraint as the configurations now must not contain any redundant points.

If the chain is indeed not irreducible, then the algorithms converge to a restriction of the limiting distribution to some connected component of the state space. The limiting distribution thus depends on the initial condition and never equals to the desired distribution.
